# -*- coding: utf-8 -*-
# ì‹¤í–‰: streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
import os, re, json
from typing import List, Dict, Any
import streamlit as st
import plotly.graph_objects as go

# --- ì„±ëŠ¥: ë¡œì»¬ CPU ë°°ì¹˜ ì¶”ë¡  ---
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# (ì„ íƒ) ìŠ¤ë ˆë“œ ìˆ˜ íŠœë‹: ê³¼í•˜ë©´ ì˜¤íˆë ¤ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ
try:
    torch.set_num_threads(int(os.getenv("TORCH_NUM_THREADS", "2")))
except Exception:
    pass

# =========================
# ê¸°ë³¸ UI ì„¤ì •
# =========================
st.set_page_config(page_title="ì†Œì„¤ ê°ì • ê³¡ì„  (ë¡œì»¬ ML, ë°°ì¹˜ ì¶”ë¡ )", layout="wide")
st.title("ğŸ“– ì†Œì„¤ í…ìŠ¤íŠ¸ ê°ì • ê³¡ì„  Â· ë¡œì»¬ ML(ë°°ì¹˜) ì „ìš©")
st.caption("Inference APIë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¡œì»¬ CPUì—ì„œ ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

st.markdown(
    "- ê¸°ë³¸ ëª¨ë¸: `dlckdfuf141/korean-emotion-kluebert-v2` (KLUE-BERT ê¸°ë°˜, 7ë¼ë²¨)\n"
    "- íŒŒì´í”„ë¼ì¸ ëŒ€ì‹  **ì§ì ‘ ë°°ì¹˜ ì¶”ë¡ **(tokenizer+model)ìœ¼ë¡œ ì†ë„ ìµœì í™”\n"
    "- BERT 512 í† í° í•œê³„ë¥¼ ê³ ë ¤í•´ `max_length`ë¥¼ ì¤„ì—¬ CPU ì†ë„ë¥¼ ëŒì–´ì˜¬ë¦½ë‹ˆë‹¤."
)

# =========================
# ë¼ë²¨ â†” ê°€ì¤‘ì¹˜(Valence) ë§¤í•‘
# =========================
DEFAULT_VALENCE = {
    "í–‰ë³µ": +1.0,
    "ì¤‘ë¦½": 0.0,
    "ë†€ëŒ": 0.0,   # ë§¥ë½ ë”°ë¼ ìƒí•˜ ê°€ëŠ¥ â†’ ì¤‘ë¦½ ì·¨ê¸‰
    "ê³µí¬": -0.7,
    "ë¶„ë…¸": -0.9,
    "ìŠ¬í””": -0.8,
    "í˜ì˜¤": -0.85,
}

# =========================
# ë¬¸ì¥/ì²­í¬ ìœ í‹¸
# =========================
SENT_SPLIT_RE = re.compile(r"(?<=[\.!?â€¦\n])\s+|(?<=ë‹¤\.)\s+|(?<=ìš”\.)\s+", re.M)

def split_sentences_kr(text: str) -> List[str]:
    sents = [s.strip() for s in SENT_SPLIT_RE.split(text.strip()) if s.strip()]
    return sents

def make_chunks(text: str, max_chars=380, overlap=80) -> List[str]:
    """
    ë¬¸ì¥ ë‹¨ìœ„ë¡œ max_chars ë‚´ì™¸ ì²­í¬ ìƒì„±(+ê²¹ì¹¨).
    BERT 512 í† í° ì œí•œ ê³ ë ¤ â†’ 320~384 í† í° ëª©í‘œë¡œ ë¬¸ì ê¸°ì¤€ 350~450 ì¶”ì²œ.
    """
    sents = split_sentences_kr(text)
    chunks, cur, cur_len = [], [], 0
    for s in sents:
        L = len(s)
        if cur_len + L + 1 <= max_chars:
            cur.append(s); cur_len += L + 1
        else:
            if cur:
                chunks.append(" ".join(cur))
            tail = chunks[-1][-overlap:] if chunks else ""
            cur = [tail + (" " if tail else "") + s]
            cur_len = len(cur[0])
    if cur:
        chunks.append(" ".join(cur))
    return chunks

# =========================
# ë¡œì»¬ ë¶„ë¥˜ê¸° (ë°°ì¹˜ ì¶”ë¡ )
# =========================
@st.cache_resource
def load_cls_model(model_id: str):
    tok = AutoTokenizer.from_pretrained(model_id)
    mdl = AutoModelForSequenceClassification.from_pretrained(model_id)
    mdl.eval()
    return tok, mdl

def classify_batch(
    texts: List[str],
    model_id: str,
    max_length: int = 320,
    batch_size: int = 16
) -> List[List[Dict[str, float]]]:
    """
    ë¡œì»¬ CPU ë°°ì¹˜ ë¶„ë¥˜. ë°˜í™˜ì€ pipeline(text-classification)ê³¼ í˜¸í™˜ë˜ëŠ” í˜•íƒœ:
    ê° ì…ë ¥ë§ˆë‹¤ [{"label":..., "score":...}, ...]
    """
    tok, mdl = load_cls_model(model_id)
    results: List[List[Dict[str, float]]] = []
    with torch.inference_mode():
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            enc = tok(
                batch,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )
            logits = mdl(**enc).logits  # [B, C]
            probs = F.softmax(logits, dim=-1).cpu().numpy()  # [B, C]
            labels = [mdl.config.id2label[j] for j in range(len(mdl.config.id2label))]
            for row in probs:
                results.append([
                    {"label": lab, "score": float(sc)} for lab, sc in zip(labels, row)
                ])
    return results

# =========================
# ì ìˆ˜í™”/ìŠ¤ë¬´ë”©/ì‹œê°í™” ìœ í‹¸
# =========================
def to_valence_score(api_output: List[Dict[str, Any]], valence_map: Dict[str, float]) -> Dict[str, Any]:
    """
    [{'label':'í–‰ë³µ','score':0.87}, ...] â†’ ê°€ì¤‘ í•©(ì—°ì†ê°’), top1 ë“± ê³„ì‚°
    ë¼ë²¨ì´ ë§¤í•‘ì— ì—†ìœ¼ë©´ 0 ê°€ì¤‘ì¹˜.
    """
    if not api_output:
        return {"weighted": 0.0, "top": "ì¤‘ë¦½", "probs": {}}

    weighted = 0.0
    per_label = {}
    for item in api_output:
        lbl = str(item.get("label", "")).strip()
        sc = float(item.get("score", 0.0))
        if not lbl:
            continue
        per_label[lbl] = sc
        weighted += sc * float(valence_map.get(lbl, 0.0))

    if not per_label:
        return {"weighted": 0.0, "top": "ì¤‘ë¦½", "probs": {}}

    top = max(per_label, key=per_label.get)
    return {"weighted": float(weighted), "top": top, "probs": per_label}

def smooth_moving_avg(vals: List[float], k: int = 5) -> List[float]:
    if k <= 1:
        return vals
    out = []
    for i in range(len(vals)):
        s = vals[max(0, i-k+1): i+1]
        out.append(sum(s)/len(s))
    return out

def plot_timeline(x, raw_vals, smoothed=None, k=1):
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=x, y=raw_vals, mode="lines+markers", name="ì›ì‹œ ì ìˆ˜"))
    if smoothed is not None and k > 1:
        fig.add_trace(go.Scatter(x=x, y=smoothed, mode="lines", name=f"ì´ë™í‰ê· (k={k})"))
    fig.add_hline(y=0, line_dash="dash")
    fig.update_layout(
        title="ì‘í’ˆ íë¦„ì— ë”°ë¥¸ ê°ì • ê³¡ì„  (0=ì¤‘ë¦½, +ê¸ì •/âˆ’ë¶€ì •)",
        xaxis_title="ì‘í’ˆ íë¦„ (ì²­í¬ ë²ˆí˜¸)",
        yaxis_title="ê°ì • ì ìˆ˜ (âˆ’1 ~ +1)",
        height=520
    )
    st.plotly_chart(fig, use_container_width=True)

# =========================
# ì‚¬ì´ë“œë°” ì˜µì…˜
# =========================
st.sidebar.header("ì˜µì…˜")
MODEL_ID = st.sidebar.text_input(
    "ëª¨ë¸ ID (text-classification í˜¸í™˜)",
    value="dlckdfuf141/korean-emotion-kluebert-v2",
    help="ì˜ˆ: dlckdfuf141/korean-emotion-kluebert-v2 (7ë¼ë²¨)"
)
max_chars = st.sidebar.slider("ì²­í¬ ê¸¸ì´(ë¬¸ì)", 250, 600, 380, 10)
overlap = st.sidebar.slider("ê²¹ì¹¨(ë¬¸ì)", 0, 200, 80, 10)
max_length = st.sidebar.slider("í† í° ìµœëŒ€ ê¸¸ì´(max_length)", 128, 512, 320, 32)
batch_size = st.sidebar.slider("ë°°ì¹˜ í¬ê¸°", 4, 64, 16, 4)
smooth_k = st.sidebar.slider("ì´ë™í‰ê·  ìœˆë„", 1, 21, 5, 2)

st.sidebar.markdown("**ë¼ë²¨-ê°€ì¤‘ì¹˜(Valence) ë§¤í•‘(JSON)**")
valence_text = st.sidebar.text_area(
    "ì˜ˆ: {\"í–‰ë³µ\":1.0, \"ì¤‘ë¦½\":0.0, \"ìŠ¬í””\":-0.8, ...}",
    json.dumps(DEFAULT_VALENCE, ensure_ascii=False, indent=2),
    height=200
)
try:
    USER_VALENCE = json.loads(valence_text)
    if isinstance(USER_VALENCE, dict) and USER_VALENCE:
        VALENCE = {k: float(v) for k, v in USER_VALENCE.items()}
    else:
        VALENCE = DEFAULT_VALENCE.copy()
except Exception:
    st.sidebar.warning("Valence JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©")
    VALENCE = DEFAULT_VALENCE.copy()

# =========================
# ì…ë ¥
# =========================
txt = st.text_area(
    "ë¶„ì„í•  ì†Œì„¤ í…ìŠ¤íŠ¸(ì „ë¬¸ ë¶™ì—¬ë„£ê¸° ê°€ëŠ¥, ìë™ ë¶„í• ):",
    height=280,
    placeholder="ì†Œì„¤ ì „ë¬¸ ë˜ëŠ” ê¸´ ë¬¸ë‹¨ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”."
)

colA, colB = st.columns([1,1])
with colA:
    show_table = st.checkbox("ì²­í¬ë³„ ë¼ë²¨ í‘œ ë³´ê¸°", value=False)
with colB:
    show_probs = st.checkbox("ë¼ë²¨ í™•ë¥  ë³´ê¸°", value=False)

run = st.button("ğŸ§  ê°ì • ë¶„ì„ & ì‹œê°í™” ì‹¤í–‰")

# =========================
# ì‹¤í–‰
# =========================
if run:
    if not txt.strip():
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    # 1) ë¶„í• 
    chunks = make_chunks(txt, max_chars=max_chars, overlap=overlap)
    if not chunks:
        st.error("ë¶„í•  ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë” ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()
    st.success(f"ìë™ ë¶„í•  ì™„ë£Œ: ì´ {len(chunks)}ê°œ ì²­í¬")

    # 2) ë°°ì¹˜ ë¶„ë¥˜ (ë¡œì»¬ CPU)
    with st.spinner("ë¡œì»¬ ë°°ì¹˜ ì¶”ë¡  ì¤‘..."):
        all_outputs = classify_batch(
            chunks,
            model_id=MODEL_ID,
            max_length=max_length,
            batch_size=batch_size
        )

    # 3) ì ìˆ˜í™” ë° í‘œ ì¤€ë¹„
    timeline_vals, top_labels, rows = [], [], []
    seen_labels = set()
    for i, (ch, out) in enumerate(zip(chunks, all_outputs), 1):
        sc = to_valence_score(out, VALENCE)
        timeline_vals.append(sc["weighted"])
        top_labels.append(sc["top"])
        seen_labels.update(sc["probs"].keys())

        row = {
            "idx": i,
            "text": ch[:120] + "..." if len(ch) > 120 else ch,
            "score": round(sc["weighted"], 4),
            "top": sc["top"],
        }
        if show_probs:
            pl = sorted(sc["probs"].items(), key=lambda x: -x[1])[:3]
            row["probs_top3"] = ", ".join([f"{k}:{v:.2f}" for k, v in pl])
        rows.append(row)

    # 4) ì‹œê°í™”
    smoothed = smooth_moving_avg(timeline_vals, k=smooth_k)
    x = list(range(1, len(chunks)+1))
    plot_timeline(x, timeline_vals, smoothed if smooth_k > 1 else None, k=smooth_k)

    # 5) í‘œ(ì„ íƒ)
    if show_table:
        st.subheader("ì²­í¬ë³„ ì£¼ìš” ë¼ë²¨/ì ìˆ˜")
        st.dataframe(rows, use_container_width=True)

    # 6) ë¼ë²¨ ì§„ë‹¨
    with st.expander("ë¼ë²¨ ì§„ë‹¨"):
        st.write("ëª¨ë¸ì´ ì‹¤ì œë¡œ ë°˜í™˜í•œ ë¼ë²¨:", sorted(seen_labels))
        missing = [l for l in seen_labels if l not in VALENCE]
        if missing:
            st.warning(f"VALENCE ë§¤í•‘ì— ì—†ëŠ” ë¼ë²¨: {missing} â†’ ë§¤í•‘ì— ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")

    # 7) ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
    result = {
        "model": MODEL_ID,
        "valence_mapping": VALENCE,
        "chunks": chunks,
        "timeline_scores": timeline_vals,
        "smoothed": smoothed,
        "top_labels": top_labels,
        "rows": rows
    }
    st.download_button(
        "ğŸ“¥ ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ",
        data=json.dumps(result, ensure_ascii=False, indent=2),
        file_name="novel_emotion_timeline_local.json",
        mime="application/json"
    )
